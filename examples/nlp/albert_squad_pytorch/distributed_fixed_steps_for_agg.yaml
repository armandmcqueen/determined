# After fine-tuning for 150 steps, model should achieve F1 = 88.52 per https://github.com/huggingface/transformers/tree/master/examples/question-answering
description: ALBert_SQuAD_PyTorch_64gpu_more_steps
hyperparameters:
    global_batch_size: 128
    learning_rate: 2.1e-4
    model_type: 'albert'
    do_lower_case: true
    adam_epsilon: 1e-8
    weight_decay: 0
    num_warmup_steps: 1080
    max_seq_length: 384
    doc_stride: 128
    max_query_length: 64
    n_best_size: 20
    max_answer_length: 30
    null_score_diff_threshold: 0.0
    max_grad_norm: 1.0
    num_training_steps: 2064 # This is the number of optimizer steps. Train for 2 epochs
    use_radam: false
resources:
    slots_per_trial: 64
searcher:
    name: single
    metric: f1
    max_length:
        records: 264396
    smaller_is_better: false
min_validation_period:
    records: 80000
reproducibility:
  experiment_seed: 1600666494
data:
    pretrained_model_name: "albert-xxlarge-v2"
    download_data: False
    task: "SQuAD2.0"  # SQuaD 2.0 has 132198 example.
entrypoint: model_def:AlbertSQuADPyTorch
optimizations:
    aggregation_frequency: 2
max_restarts: 0
bind_mounts:
    - host_path: /home/ubuntu/dtrain-fsx/albert-cache/
      container_path: /mnt/data
      read_only: false

